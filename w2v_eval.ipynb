{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v_eval.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_r31y70AbK3"
      },
      "source": [
        "from utils import *\n",
        "from cluster import *\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import codecs\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import scipy\n",
        "import os, json\n",
        "import operator\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiCtCA6Pthcx"
      },
      "source": [
        "Loading word2vec embeddings\n",
        "w2v, w2v_w2i, w2v_vocab = load_w2v(\"data/GoogleNews-vectors-negative300.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-zRGc4ErWN7"
      },
      "source": [
        "***Plain word2vec WEAT***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7sRhhnzrCBB"
      },
      "source": [
        "# WEAT embeddings\n",
        "# load word lists for tests\n",
        "# start with the male and female names\n",
        "A = [name.lower() for name in WEAT_words['A']]\n",
        "B = [name.lower() for name in WEAT_words['B']]\n",
        "# career and family\n",
        "C = WEAT_words['C']\n",
        "D = WEAT_words['D']\n",
        "# math and arts\n",
        "E = WEAT_words['E']\n",
        "F = WEAT_words['F']\n",
        "# science and arts\n",
        "G = WEAT_words['G']\n",
        "H = WEAT_words['H']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0da6ljcrSv1"
      },
      "source": [
        "# calculate effect size and p value for career and family plain w2v\n",
        "print(\"PLAIN W2V\")\n",
        "print('Career and Family')\n",
        "print(\"Effect size:\",effect_size(A, B, C, D, w2v, w2v_w2i, w2v_vocab))\n",
        "print(\"P-value:\",p_value_test(A, B, C, D, w2v, w2v_w2i, w2v_vocab))\n",
        "\n",
        "# calculate effect size and p value for math and arts plain w2v\n",
        "print('Math and Arts')\n",
        "print(\"Effect size:\",effect_size(A, B, E, F, w2v, w2v_w2i, w2v_vocab))\n",
        "print(\"P-value:\",p_value_test(A, B, E, F, w2v, w2v_w2i, w2v_vocab))\n",
        "\n",
        "# calculate effect size and p value for science and arts plain w2v\n",
        "print('Science and Arts')\n",
        "print(\"Effect size:\",effect_size(A, B, G, H, w2v, w2v_w2i, w2v_vocab))\n",
        "print(\"P-value:\",p_value_test(A, B, G, H, w2v, w2v_w2i, w2v_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK6sp3NbCgjZ"
      },
      "source": [
        "***Clustering for plain Word2Vec***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrUIgde_Cflf"
      },
      "source": [
        "# limit vocab by excluding words that 'should' have gender bias\n",
        "gender_specific = []\n",
        "\n",
        "with open(\"data/male_words.txt\") as f:\n",
        "    for l in f:\n",
        "        gender_specific.append(l.strip())\n",
        "with open(\"data/female_words.txt\") as f:\n",
        "    for l in f:\n",
        "        gender_specific.append(l.strip())\n",
        "\n",
        "with codecs.open(\"data/gender_specific_full.json\") as f:\n",
        "    gender_specific.extend(json.load(f))\n",
        "\n",
        "w2v = decentralize(w2v)\n",
        "w2v = normalize(w2v)\n",
        "w2v_vocab_limit, w2v_wv_limit, w2v_w2i_limit = limit_vocab(w2v, w2v_w2i, w2v_vocab, exclude=gender_specific)\n",
        "# dbl_glove_vocab_limit, dbl_glove_wv_limit, dbl_glove_w2i_limit = limit_vocab(dbl_glove_wv, dbl_glove_w2i, dbl_glove_vocab, exclude=gender_specific)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfyEwO7mDd5X"
      },
      "source": [
        "# get most biased words\n",
        "he_vector = w2v[w2v_w2i['he'], :]\n",
        "she_vector = w2v[w2v_w2i['she'], :]\n",
        "biased_words = compute_word_bias(w2v_wv_limit, w2v_w2i_limit, w2v_vocab_limit, he_vector, she_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7s-qEUNGQMW"
      },
      "source": [
        "# cluster using limited vocabulary\n",
        "for n in [100, 500, 1000]:\n",
        "  print(\"\\n Precision for plain word2vec for\",n,\"words\")\n",
        "  my_cluster(w2v_wv_limit, w2v_w2i_limit, 1, w2v_vocab_limit, biased_words, num_biased_words=n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbcMISn88bJT"
      },
      "source": [
        "# LOAD HARD-DEBIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlQD5X0cr4E3"
      },
      "source": [
        "from hard_debias import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHuuBDbNtZ4y"
      },
      "source": [
        "hard_w2v = hard_debias_w2v(w2v, w2v_w2i, w2v_vocab,\"data/definitional_pairs.json\")\n",
        "print(\"Hard-debiased vectors loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6kf8e0e9Bs6"
      },
      "source": [
        "WEAT FOR HARD-DEBIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp0KtrMC2E84"
      },
      "source": [
        "# calculate effect size and p value for career and family hard w2v\n",
        "print(\"HARD-DEBIASED W2V\")\n",
        "print('Career and Family')\n",
        "print(\"Effect-size:\",effect_size(A, B, C, D, hard_w2v, w2v_w2i, w2v_vocab))\n",
        "print(\"P-value:\",p_value_test(A, B, C, D, hard_w2v, w2v_w2i, w2v_vocab))\n",
        "\n",
        "# calculate effect size and p value for math and arts hard w2v\n",
        "print('Math and Arts')\n",
        "print(\"Effect-size:\",effect_size(A, B, E, F, hard_w2v, w2v_w2i, w2v_vocab))\n",
        "print(\"P-value:\",p_value_test(A, B, E, F, hard_w2v, w2v_w2i, w2v_vocab))\n",
        "\n",
        "# calculate effect size and p value for science and arts hard w2v\n",
        "print('Science and Arts')\n",
        "print(\"Effect-size:\",effect_size(A, B, G, H, hard_w2v, w2v_w2i, w2v_vocab))\n",
        "print(\"P-value:\",p_value_test(A, B, G, H, hard_w2v, w2v_w2i, w2v_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2xkEVopFDDK"
      },
      "source": [
        "***Clustering for hard debiased Word2Vec***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mslVzO6eFN7w"
      },
      "source": [
        "# limit vocab by excluding words that 'should' have gender bias\n",
        "gender_specific = []\n",
        "\n",
        "with open(\"data/male_words.txt\") as f:\n",
        "    for l in f:\n",
        "        gender_specific.append(l.strip())\n",
        "with open(\"data/female_words.txt\") as f:\n",
        "    for l in f:\n",
        "        gender_specific.append(l.strip())\n",
        "\n",
        "with codecs.open(\"data/gender_specific_full.json\") as f:\n",
        "    gender_specific.extend(json.load(f))\n",
        "\n",
        "hard_w2v = decentralize(hard_w2v)\n",
        "hard_w2v = normalize(hard_w2v)\n",
        "hard_w2v_vocab_limit, hard_w2v_wv_limit, hard_w2v_w2i_limit = limit_vocab(hard_w2v, w2v_w2i, w2v_vocab, exclude=gender_specific)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OV_ogP1FhRK"
      },
      "source": [
        "# get most biased words\n",
        "he_vector = hard_w2v[w2v_w2i['he'], :]\n",
        "she_vector = hard_w2v[w2v_w2i['she'], :]\n",
        "biased_words = compute_word_bias(hard_w2v_wv_limit, hard_w2v_w2i_limit, hard_w2v_vocab_limit, he_vector, she_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3s7CKqFyw6"
      },
      "source": [
        "# cluster using limited vocabulary\n",
        "for n in [100, 500, 1000]:\n",
        "  print(\"\\n Precision for hard-debiased word2vec for\",n,\"words\")\n",
        "  my_cluster(hard_w2v_wv_limit, hard_w2v_w2i_limit, 1, hard_w2v_vocab_limit, biased_words, num_biased_words=n)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
